# -*- coding: utf-8 -*-
"""Cópia de Cópia de analisar_pdf_e_extrair_ccb_parcelas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ft_x02hmu_GbHWtnidI6uCqHpYhcYAJf
"""

# ===============================================
# 1. Instalar bibliotecas
# ===============================================
!pip install -q PyMuPDF pandas

import fitz  # PyMuPDF
import pandas as pd
import re
import zipfile
import os
from google.colab import files

# ===============================================
# 2. Upload do arquivo ZIP
# ===============================================
print("Faça upload do arquivo ZIP contendo vários PDFs")
uploaded = files.upload()
zip_path = list(uploaded.keys())[0]

# ===============================================
# 3. Descompactar os PDFs
# ===============================================
extract_folder = "/content/pdfs_extraidos"
os.makedirs(extract_folder, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

pdf_files = [os.path.join(extract_folder, f) for f in os.listdir(extract_folder) if f.lower().endswith(".pdf")]
print(f"Total de PDFs encontrados: {len(pdf_files)}")

# ===============================================
# 4. Função para extrair informações de cada PDF
# ===============================================
import fitz  # PyMuPDF
import pandas as pd
import re
import os

def extrair_informacoes(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        texto_completo = ""
        for page in doc:
            texto_completo += page.get_text("text")
        doc.close()

        # --- NOVA ETAPA DE LIMPEZA ---
        # Lista de padrões de texto (cabeçalhos/rodapés) a serem removidos antes da extração.
        padroes_para_remover = [
            re.compile(r'^\s*https://www.reclameaqui.com.br/.*$'),
            re.compile(r'^\s*\d+/\d+\s*$'),  # Padrão de página (ex: 9/11)
            re.compile(r'^\s*\d{2}/\d{2}/\d{4},\s*\d{2}:\d{2}\s*$'), # Padrão de data/hora do cabeçalho
            re.compile(r'^\s*Reclame Aqui - Pesquise antes de comprar\. Reclame\. Resolva\s*$'),
            re.compile(r'^\s*Gere relatórios personalizados.*$'),
            re.compile(r'^\s*Responder\s*$'),
            re.compile(r'^\s*Sem avaliação\s*$'),
            re.compile(r'^\s*Não respondida\s*$')
        ]

        linhas_originais = texto_completo.split('\n')
        linhas_limpas = []
        for linha in linhas_originais:
            # Se a linha não corresponder a nenhum padrão de lixo, nós a mantemos.
            if not any(pattern.match(linha.strip()) for pattern in padroes_para_remover):
                linhas_limpas.append(linha)

        texto_limpo = "\n".join(linhas_limpas)
        # --- FIM DA ETAPA DE LIMPEZA ---

        # O restante da lógica de extração agora roda no 'texto_limpo'
        id_regex = re.compile(r'ID:\s*(\d{9})')
        data_regex = re.compile(r'(\d{2}/\d{2}/\d{2}\s*-\s*\d{2}:\d{2})')
        via_regex = re.compile(r'(Via\s*(?:site|mobile|app))')
        local_regex = re.compile(r'([A-Za-z\u00C0-\u017F\s\']+,\s*[A-Z]{2})')

        reclamacoes_texto = re.split(r'\n(?=ID:)', texto_limpo)

        dados_extraidos = []

        for bloco_str in reclamacoes_texto:
            if not id_regex.search(bloco_str):
                continue

            id_match = id_regex.search(bloco_str)
            data_match = data_regex.search(bloco_str)
            via_match = via_regex.search(bloco_str)
            local_match = local_regex.search(bloco_str)

            id_val = id_match.group(1) if id_match else None
            data_val = data_match.group(1) if data_match else None
            via_val = via_match.group(1) if via_match else None
            local_val = local_match.group(1) if local_match else None

            nome_val = None
            descricao_val = ""

            linhas_bloco = [l.strip() for l in bloco_str.strip().split('\n') if l.strip()]

            if data_val:
                try:
                    indice_data = next(i for i, linha in enumerate(linhas_bloco) if data_val in linha)
                    # A lógica agora é mais segura pois o texto está limpo.
                    # O nome deve ser a linha anterior à data E à via.
                    # A descrição são as linhas entre o ID e o Nome.
                    if indice_data > 0:
                        # Assumimos que o nome é a última linha antes dos metadados (data, via, local)
                        # Geralmente, a estrutura é: ID, Desc, Nome, Data, Via, Local
                        nome_val = linhas_bloco[indice_data - 1]

                        linha_inicial_desc = 1 if id_regex.match(linhas_bloco[0]) else 0
                        descricao_linhas = linhas_bloco[linha_inicial_desc:indice_data - 1]
                        descricao_val = " ".join(descricao_linhas).strip()

                except StopIteration:
                    pass

            dados_extraidos.append({
                'ID': id_val,
                'Arquivo': os.path.basename(pdf_path),
                'Nome': nome_val,
                'DataHora': data_val,
                'Via': via_val,
                'Local': local_val,
                'Descricao': descricao_val
            })

        return dados_extraidos

    except Exception as e:
        print(f"Erro ao processar o arquivo {pdf_path}: {e}")
        return []
# ===============================================
# 5. Extrair informações de todos os PDFs
# ===============================================
dados_totais = []
for pdf in pdf_files:
    info = extrair_informacoes(pdf)
    dados_totais.extend(info)

# ===============================================
# 6. Criar DataFrame final
# ===============================================
if dados_totais:
    df = pd.DataFrame(dados_totais)

    print(df.head(10))
else:
    print("Nenhuma informação extraída.")

# ===============================================
# 6. Salvar em Excel e permitir download
# ===============================================
if dados_totais:
    output_excel = "/content/extracao_celulas.xlsx"
    df.to_excel(output_excel, index=False, engine='openpyxl')
    files.download(output_excel)